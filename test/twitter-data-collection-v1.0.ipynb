{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API as api\n",
    "from tweepy import Cursor\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os, sys\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term files ['.ipynb_checkpoints', 'meghan.csv', 'miso.csv', 'racisttest.csv']\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Initializing from file failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f8e8f67ea511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mterms_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mterms\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../media/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f8e8f67ea511>\u001b[0m in \u001b[0;36mget_hate_terms\u001b[1;34m(csv_file_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m '''\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf'\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m### option for manual input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#terms.sort_values('term', inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#print('Before removing duplicated terms: ', terms.count())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Initializing from file failed"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get a list of hateful terms. These ae collected from hatebase.org using the hatespeech-vocabulary-collection script \n",
    "stored in hatespeech-terms.csv file\n",
    "csv_file_name should include the full path to the file\n",
    "the file should be formatted as one column labelled 'term' and as many terms as required\n",
    "\n",
    "term\n",
    "term 1\n",
    "term 2\n",
    "term 3\n",
    "...\n",
    "'''\n",
    "def get_hate_terms(csv_file_name):\n",
    "    terms = pd.read_csv(csv_file_name, encoding='utf')   ### option for manual input\n",
    "    #terms.sort_values('term', inplace=True)\n",
    "    #print('Before removing duplicated terms: ', terms.count())\n",
    "    terms.drop_duplicates(keep='first', inplace=True)\n",
    "    #print('After removing duplicated terms: ',  terms.count())\n",
    "    return terms['terms'].astype(str).values.tolist()\n",
    "\n",
    "# read the terms from a folder\n",
    "# the terms_files directory should include one or more csv files with terms in them\n",
    "terms_file = os.listdir('../media')\n",
    "print ('term files', terms_file)\n",
    "terms = []\n",
    "for i in terms_file:\n",
    "    terms += get_hate_terms('../media/'+i)\n",
    "\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Twitter only accepts n number of terms at a time\n",
    "Use this function if we have a large number of terms so we supply twitter with small list of terms in each API call\n",
    "return randomised list of terms\n",
    "'''\n",
    "def get_list_of_list(list_of_terms, sublist_size):\n",
    "    random_list = random.sample(list_of_terms, len(list_of_terms)) \n",
    "    for i in range(0, len(random_list), sublist_size):\n",
    "        # Create an index range for l of n items:\n",
    "        yield random_list[i:i+sublist_size]\n",
    " \n",
    "term_list = list(get_list_of_list(terms, 10))\n",
    "print('Number of lists of terms to process: %d'%len(term_list))\n",
    "for t in term_list:\n",
    "    print('Start time on the following list %s of length %d'%(time.strftime(\"%Y-%m-%d, %H:%M:%S\"), len(t)))\n",
    "    print(t)\n",
    "    \n",
    "print('Finished time %s'%time.strftime(\"%Y-%m-%d, %H:%M:%S\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accesstoken = '1192071683483557888-JsajbXeIZV4yO7heVLBiyKoLmcwE2D'\n",
    "accesstokensecret = 'iHopL1lf1l7vYQw13sbtNpumfRyjJ3JOoPXMwtadj8uSm'\n",
    "consumerkey = '4nmllrY1s4k2x8ClNEMA4Ohvd'\n",
    "consumersecret = 'GBmKtYX2wrGVDxbGn2kPbYbuD4wUGEq2i6RVI1dzT8jCmgGxYX'\n",
    " \n",
    "authorization = OAuthHandler(consumerkey, consumersecret)\n",
    "authorization.set_access_token(accesstoken, accesstokensecret)\n",
    "\n",
    "class MyStreamListener(StreamListener):\n",
    "\n",
    "    def __init__(self, output_file):\n",
    "        self.output_file = output_file\n",
    "        self.siesta = 0\n",
    "        self.nightnight = 0\n",
    "        self.start_time = time.time()\n",
    "        self.time_limit = time_limit\n",
    "        self.tweet_file = open(self.output_file, 'a')\n",
    "        \n",
    "        super(MyStreamListener, self).__init__()\n",
    "\n",
    "    def on_status(self, status):\n",
    "        if (time.time() - self.start_time) < self.time_limit:\n",
    "            try:\n",
    "                if 'extended_tweet' in status._json:\n",
    "                    json.dump(status._json, self.tweet_file)\n",
    "                    self.tweet_file.write(\"%s\"%'\\n')\n",
    "            except  Exception as e:\n",
    "                print('failed on writing data to file:', e)\n",
    "                pass\n",
    "            return True\n",
    "        else:\n",
    "            self.tweet_file.close()\n",
    "            return False\n",
    "           \n",
    "    def on_error(self, status_code):\n",
    "        print('Error:', str(status_code))\n",
    "        if status_code == 420:\n",
    "            sleepy = 60 * math.pow(2, self.siesta)\n",
    "            print(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "            print(\"A reconnection attempt will occur in \" + str(sleepy/60) + \" minutes.\")\n",
    "            print('''\n",
    "            *******************************************************************\n",
    "            From Twitter Streaming API Documentation\n",
    "            420: Rate Limited\n",
    "            The client has connected too frequently. For example, an \n",
    "            endpoint returns this status if:\n",
    "            - A client makes too many login attempts in a short period \n",
    "              of time.\n",
    "            - Too many copies of an application attempt to authenticate \n",
    "              with the same credentials.\n",
    "            *******************************************************************\n",
    "            ''')\n",
    "            time.sleep(sleepy)\n",
    "            self.siesta += 1\n",
    "        else:\n",
    "            sleepy = 5 * math.pow(2, self.nightnight)\n",
    "            print(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "            print('A reconnection attempt will occur in ', str(sleepy), ' seconds.')\n",
    "            time.sleep(sleepy)\n",
    "            self.nightnight += 1\n",
    "        return True     \n",
    "        \n",
    "    def on_limit(self, track):\n",
    "        sleepy = 5 * math.pow(2, self.nightnight)\n",
    "        print(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        print('A reconnection attempt will occur in ', str(sleepy), ' seconds.')\n",
    "        time.sleep(sleepy)\n",
    "        self.nightnight += 1\n",
    "        sys.stderr.write(\"on_limit  \" + track + \"\\n\")\n",
    "        return True\n",
    "    \n",
    "    def on_timeout(self):\n",
    "        sleepy = 5 * math.pow(2, self.nightnight)\n",
    "        print(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        print('Timeout ', str(sleepy), ' seconds.')\n",
    "        time.sleep(sleepy)\n",
    "        self.nightnight += 1\n",
    "        return True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#terms = get_hate_terms(terms)\n",
    "term_list = list(get_list_of_list(terms, 10))\n",
    "time_limit = 360 # in seconds\n",
    "\n",
    "# the following goes through the list of terms, which is a list of list\n",
    "# each sublist takes about 6 minutes, you can change the time limit for each list by resetting the time_limit variable value\n",
    "# we feed all the sublists (the full terms in the csv file) in onr hous\n",
    "for i in range(1, 1000): \n",
    "    print('Number of lists of terms to process: %d\\n'%len(term_list))\n",
    "    print(i, ' out of  1000  repition.\\n')\n",
    "    k = 0\n",
    "    for terms in term_list:\n",
    "        k += 1\n",
    "        print('\\t', k, ' out of  ',len(term_list),'  sets of terms.\\n')\n",
    "        print('Start time on the following list %s\\n'%time.strftime(\"%Y-%m-%d, %H:%M:%S\"))\n",
    "        print(terms,'\\n')\n",
    "        start_time = time.time() \n",
    "        \n",
    "        try:\n",
    "            twitterStream = Stream(authorization, MyStreamListener('./data/tweets.json'))\n",
    "            twitterStream.filter(track=terms, languages=[\"en\"], stall_warnings=True)\n",
    "            if (time.time() - start_time) < time.time()+time_limit:\n",
    "                \n",
    "                #print('------------sleeping for 3 seconds------------------')\n",
    "                time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(e.__doc__) \n",
    "        \n",
    "    #time.sleep(905)    \n",
    "    print('Finished round: %d at time %s'%(i, time.strftime(\"%Y-%m-%d, %H:%M:%S\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
