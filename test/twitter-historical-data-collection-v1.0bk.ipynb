{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API as api\n",
    "from tweepy import Cursor\n",
    "import tweepy\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os, sys\n",
    "import pprint\n",
    "import math\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of list terms to be processed: 1\n",
      "['Meghan Markle', '#HarryandMeghan', 'Meghan']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get a list of hateful terms. These ae collected from hatebase.org using the hatespeech-vocabulary-collection script \n",
    "stroed in hatespeech-terms.csv file\n",
    "csv_file_name should include the full path to the file\n",
    "the file should be formatted as one column labelled 'term' and as many terms as required\n",
    "\n",
    "term\n",
    "term 1\n",
    "term 2\n",
    "term 3\n",
    "...\n",
    "'''\n",
    "def get_hate_terms(csv_file_name):\n",
    "    terms = pd.read_csv(csv_file_name, encoding='utf')\n",
    "    #terms.sort_values('term', inplace=True)\n",
    "    #print('Before removing duplicated terms: ', terms.count())\n",
    "    terms.drop_duplicates(keep='first', inplace=True)\n",
    "    #print('After removing duplicated terms: ',  terms.count())\n",
    "    return terms['terms'].astype(str).values.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "terms = get_hate_terms('./data/user_handles/meghan.csv')\n",
    "\n",
    "'''\n",
    "Twitter only accepts n number of terms at a time\n",
    "Use this function if we have a large number of terms so we supply twitter with small list of terms in each API call\n",
    "return randomised list of terms\n",
    "'''\n",
    "def get_list_of_list(list_of_terms, sublist_size):\n",
    "    random_list = random.sample(list_of_terms, len(list_of_terms)) \n",
    "    #print(random_list)\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(random_list), sublist_size):\n",
    "        # Create an index range for l of n items:\n",
    "        yield random_list[i:i+sublist_size]\n",
    " \n",
    "term_list = list(get_list_of_list(terms, 99))\n",
    "print('The number of list terms to be processed: %d'%len(term_list))\n",
    "for terms in term_list:\n",
    "    print(terms)\n",
    "    \n",
    "def get_start_time():\n",
    "    start_time = time.time()\n",
    "    print('started at: ',datetime.fromtimestamp(start_time).strftime(\"%c\"))\n",
    "    return start_time\n",
    "   \n",
    "def get_time_taken(start_time): \n",
    "    end_time = time.time()\n",
    "    print('ended at: ',datetime.fromtimestamp(end_time).strftime(\"%c\"))\n",
    "    total_time_taken = end_time-start_time\n",
    "    total_time_taken = datetime.fromtimestamp(total_time_taken)\n",
    "    print('time taken: ', total_time_taken.strftime('%M'),':',total_time_taken.strftime('%S'), ' seconds')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from TwitterAPI import TwitterRestPager\n",
    "#from t import TwitterRestPager\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "GEOCODE = '40,74,10km'\n",
    "cons = get_hate_terms('./data/abusive_terms/miso.csv')\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search, q=SEARCH_TERM, count=100, lang=\"en\", since=\"2019-12-09\", until=\"2019-12-15\").items():\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['bitch', 'slag', 'whore', 'cunt', 'cow', 'bint', 'slut', 'shag', 'tart', 'hag', 'nag', 'witch', 'lezza', 'lesbo', 'dyke', 'trollop', 'bimbo', 'bird', 'cocktease', 'conchuda', 'feminazi', 'gash', 'ho', 'hoe', 'hoodrat', 'kunt', 'pussy', 'scank', 'skank', 'twat']\n",
      "bitch\n",
      "reached limit\n",
      "slag\n",
      "reached limit\n",
      "whore\n",
      "reached limit\n",
      "cunt\n",
      "reached limit\n",
      "cow\n",
      "reached limit\n",
      "bint\n",
      "reached limit\n",
      "slut\n",
      "reached limit\n",
      "shag\n",
      "reached limit\n",
      "tart\n",
      "reached limit\n",
      "hag\n",
      "reached limit\n",
      "nag\n",
      "reached limit\n",
      "witch\n",
      "reached limit\n",
      "lezza\n",
      "reached limit\n",
      "lesbo\n",
      "reached limit\n",
      "dyke\n",
      "reached limit\n",
      "trollop\n",
      "reached limit\n",
      "bimbo\n",
      "reached limit\n",
      "bird\n",
      "reached limit\n",
      "cocktease\n",
      "reached limit\n",
      "conchuda\n",
      "reached limit\n",
      "feminazi\n",
      "reached limit\n",
      "gash\n",
      "reached limit\n",
      "ho\n",
      "reached limit\n",
      "hoe\n",
      "reached limit\n",
      "hoodrat\n",
      "reached limit\n",
      "kunt\n",
      "reached limit\n",
      "pussy\n",
      "reached limit\n",
      "scank\n",
      "reached limit\n",
      "skank\n",
      "reached limit\n",
      "twat\n",
      "reached limit\n"
     ]
    }
   ],
   "source": [
    "accesstoken = '1192071683483557888-JsajbXeIZV4yO7heVLBiyKoLmcwE2D'\n",
    "accesstokensecret = 'iHopL1lf1l7vYQw13sbtNpumfRyjJ3JOoPXMwtadj8uSm'\n",
    "consumerkey = '4nmllrY1s4k2x8ClNEMA4Ohvd'\n",
    "consumersecret = 'GBmKtYX2wrGVDxbGn2kPbYbuD4wUGEq2i6RVI1dzT8jCmgGxYX'\n",
    "\n",
    "\n",
    " \n",
    "authorization = OAuthHandler(consumerkey, consumersecret)\n",
    "authorization.set_access_token(accesstoken, accesstokensecret)\n",
    "\n",
    "hate_terms = get_hate_terms('./data/abusive_terms.csv')\n",
    "# cons = get_hate_terms('./data/conservative.csv')\n",
    "# ind = get_hate_terms('./data/independent.csv')\n",
    "# labour =get_hate_terms('./data/labour.csv')\n",
    "# lib_dem = get_hate_terms('./data/lib_dem.csv')\n",
    " \n",
    "terms = hate_terms \n",
    "# cons + ind + labour + lib_dem\n",
    "\n",
    "print(len(terms), terms)\n",
    "time_limit = 15 * 60\n",
    "jfile = open('./data/ua3.json', 'a')\n",
    "\n",
    "api = tweepy.API(authorization, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, retry_delay=5)\n",
    "for i in terms:\n",
    "    try:\n",
    "        c = tweepy.Cursor(api.search, q=i, count=100, lang=\"en\", \n",
    "                          since=\"2019-12-9\", \n",
    "                          until=\"2019-12-17\").items()\n",
    "        print(i)\n",
    "        start_time = time.time() \n",
    "        for tweet in c:\n",
    "            json.dump(tweet._json, jfile)\n",
    "            jfile.write(\"%s\"%'\\n')\n",
    "        if (time.time() - start_time) < time.time()+time_limit:\n",
    "            #print('------------sleeping for 10 seconds------------------')\n",
    "            print('reached limit')\n",
    "            time.sleep(3)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__doc__) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
