{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API as api\n",
    "from tweepy import Cursor\n",
    "import tweepy\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os, sys\n",
    "import pprint\n",
    "import math\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./data/conservative.csv' does not exist: b'./data/conservative.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1dda42327fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#hate_terms = get_hate_terms('./data/abusive_terms.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mcons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/conservative.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/independent.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mlabour\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/labour.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1dda42327fd4>\u001b[0m in \u001b[0;36mget_hate_terms\u001b[1;34m(csv_file_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m '''\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_hate_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m#terms.sort_values('term', inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#print('Before removing duplicated terms: ', terms.count())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./data/conservative.csv' does not exist: b'./data/conservative.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get a list of hateful terms. These ae collected from hatebase.org using the hatespeech-vocabulary-collection script \n",
    "stroed in hatespeech-terms.csv file\n",
    "csv_file_name should include the full path to the file\n",
    "the file should be formatted as one column labelled 'term' and as many terms as required\n",
    "\n",
    "term\n",
    "term 1\n",
    "term 2\n",
    "term 3\n",
    "...\n",
    "'''\n",
    "def get_hate_terms(csv_file_name):\n",
    "    terms = pd.read_csv(csv_file_name, encoding='utf')\n",
    "    #terms.sort_values('term', inplace=True)\n",
    "    #print('Before removing duplicated terms: ', terms.count())\n",
    "    terms.drop_duplicates(keep='first', inplace=True)\n",
    "    #print('After removing duplicated terms: ',  terms.count())\n",
    "    return terms['terms'].astype(str).values.tolist()\n",
    "    \n",
    "\n",
    "#hate_terms = get_hate_terms('./data/abusive_terms.csv')\n",
    "cons = get_hate_terms('./data/conservative.csv')\n",
    "ind = get_hate_terms('./data/independent.csv')\n",
    "labour =get_hate_terms('./data/labour.csv')\n",
    "lib_dem = get_hate_terms('./data/lib_dem.csv')\n",
    "\n",
    "terms = cons + ind + labour + lib_dem\n",
    "\n",
    "'''\n",
    "Twitter only accepts n number of terms at a time\n",
    "Use this function if we have a large number of terms so we supply twitter with small list of terms in each API call\n",
    "return randomised list of terms\n",
    "'''\n",
    "def get_list_of_list(list_of_terms, sublist_size):\n",
    "    random_list = random.sample(list_of_terms, len(list_of_terms)) \n",
    "    #print(random_list)\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(random_list), sublist_size):\n",
    "        # Create an index range for l of n items:\n",
    "        yield random_list[i:i+sublist_size]\n",
    " \n",
    "term_list = list(get_list_of_list(terms, 99))\n",
    "print('The number of list terms to be processed: %d'%len(term_list))\n",
    "for terms in term_list:\n",
    "    print(terms)\n",
    "    \n",
    "def get_start_time():\n",
    "    start_time = time.time()\n",
    "    print('started at: ',datetime.fromtimestamp(start_time).strftime(\"%c\"))\n",
    "    return start_time\n",
    "   \n",
    "def get_time_taken(start_time): \n",
    "    end_time = time.time()\n",
    "    print('ended at: ',datetime.fromtimestamp(end_time).strftime(\"%c\"))\n",
    "    total_time_taken = end_time-start_time\n",
    "    total_time_taken = datetime.fromtimestamp(total_time_taken)\n",
    "    print('time taken: ', total_time_taken.strftime('%M'),':',total_time_taken.strftime('%S'), ' seconds')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from TwitterAPI import TwitterRestPager\n",
    "#from t import TwitterRestPager\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "GEOCODE = '40,74,10km'\n",
    "cons = get_hate_terms('./data/conservative.csv')\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search, q=SEARCH_TERM, count=100, lang=\"en\", since=\"2019-12-09\", until=\"2019-12-15\").items():\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 ['AlokSharma_RDG', 'Rehman_Chishti', 'BimAfolami', 'KwasiKwarteng', 'ShaileshVara', 'sajidjavid', 'HelenGrantMP', 'AlanMakHavant', 'JamesCleverly', 'SuellaBraverman', 'patel4witham', 'Nus_Ghani', 'nadhimzahawi', 'RishiSunak', 'KemiBadenoch', 'AdamAfriyie', 'ranil', 'JDjanogly', 'grantshapps', 'Michael_Ellis1', 'lucyfrazermp', 'Mike_Fabricant', 'ZacGoldsmith', 'VoteIvan', 'YasinForBedford', 'FaisalRashidMP', 'MarshadeCordova', 'Eleanor_SmithMP', 'Bambos4MP', 'PreetKGillMP', 'Valerie_VazMP', 'RupaHuq', 'DrRosena', 'TulipSiddiq', 'SeemaMalhotra1', 'labourlewis', 'YasminQureshiMP', 'TanDhesi', 'lisanandy', 'ChiOnwurah', 'khalid4PB', 'MpHendrick', 'JanetDaby', 'Imran_HussainMP', 'NazShahBfd', 'KateOsamor', 'VirendraSharma', 'ThangamMP', 'DawnButlerBrent', 'rushanaraali', 'HackneyAbbott', 'Afzal4Gorton', 'ShabanaMahmood', 'DavidLammy', 'SadiqKhan', 'Jeremy_Newmark', 'RuthSmeeth', 'FabianLeedsNE', 'alexsobel', 'Ed_Miliband', 'margarethodge', 'LaylaMoran', 'ChukaUmunna', 'SamGyimah', 'lucianaberger']\n",
      "AlokSharma_RDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "Rehman_Chishti\n",
      "reached limit\n",
      "BimAfolami\n",
      "reached limit\n",
      "KwasiKwarteng\n",
      "reached limit\n",
      "ShaileshVara\n",
      "reached limit\n",
      "sajidjavid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "HelenGrantMP\n",
      "reached limit\n",
      "AlanMakHavant\n",
      "reached limit\n",
      "JamesCleverly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "SuellaBraverman\n",
      "reached limit\n",
      "patel4witham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "Nus_Ghani\n",
      "reached limit\n",
      "nadhimzahawi\n",
      "reached limit\n",
      "RishiSunak\n",
      "reached limit\n",
      "KemiBadenoch\n",
      "reached limit\n",
      "AdamAfriyie\n",
      "reached limit\n",
      "ranil\n",
      "reached limit\n",
      "JDjanogly\n",
      "reached limit\n",
      "grantshapps\n",
      "reached limit\n",
      "Michael_Ellis1\n",
      "reached limit\n",
      "lucyfrazermp\n",
      "reached limit\n",
      "Mike_Fabricant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "ZacGoldsmith\n",
      "reached limit\n",
      "VoteIvan\n",
      "reached limit\n",
      "YasinForBedford\n",
      "reached limit\n",
      "FaisalRashidMP\n",
      "reached limit\n",
      "MarshadeCordova\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "Eleanor_SmithMP\n",
      "reached limit\n",
      "Bambos4MP\n",
      "reached limit\n",
      "PreetKGillMP\n",
      "reached limit\n",
      "Valerie_VazMP\n",
      "reached limit\n",
      "RupaHuq\n",
      "reached limit\n",
      "DrRosena\n",
      "reached limit\n",
      "TulipSiddiq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "SeemaMalhotra1\n",
      "reached limit\n",
      "labourlewis\n",
      "reached limit\n",
      "YasminQureshiMP\n",
      "reached limit\n",
      "TanDhesi\n",
      "reached limit\n",
      "lisanandy\n",
      "reached limit\n",
      "ChiOnwurah\n",
      "reached limit\n",
      "khalid4PB\n",
      "reached limit\n",
      "MpHendrick\n",
      "reached limit\n",
      "JanetDaby\n",
      "reached limit\n",
      "Imran_HussainMP\n",
      "reached limit\n",
      "NazShahBfd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "KateOsamor\n",
      "reached limit\n",
      "VirendraSharma\n",
      "reached limit\n",
      "ThangamMP\n",
      "reached limit\n",
      "DawnButlerBrent\n",
      "reached limit\n",
      "rushanaraali\n",
      "reached limit\n",
      "HackneyAbbott\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 792\n",
      "Rate limit reached. Sleeping for: 812\n",
      "Rate limit reached. Sleeping for: 745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "Afzal4Gorton\n",
      "reached limit\n",
      "ShabanaMahmood\n",
      "reached limit\n",
      "DavidLammy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 801\n",
      "Rate limit reached. Sleeping for: 774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "SadiqKhan\n",
      "reached limit\n",
      "Jeremy_Newmark\n",
      "reached limit\n",
      "RuthSmeeth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "FabianLeedsNE\n",
      "reached limit\n",
      "alexsobel\n",
      "reached limit\n",
      "Ed_Miliband\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "margarethodge\n",
      "reached limit\n",
      "LaylaMoran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "ChukaUmunna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "SamGyimah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n",
      "lucianaberger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached limit\n"
     ]
    }
   ],
   "source": [
    "accesstoken = '98886733-f5M5aYz2w3zivEILEHuFfvrDLXhYkYwL1xvgr9c9y'\n",
    "accesstokensecret = 'uOwbZGLE3WyevrWkKPaJu9rjtSY5IZJ3TE7pVJU8tp9ud'\n",
    "consumerkey = '2lWIAi4Z6UKKBxv33YTtpqilM'\n",
    "consumersecret = 'm7uNftueKNzg8LTG6s8WHDkMGII3DBXoXbbySnjzS0dHS6oTll'\n",
    " \n",
    "authorization = OAuthHandler(consumerkey, consumersecret)\n",
    "authorization.set_access_token(accesstoken, accesstokensecret)\n",
    "\n",
    "\n",
    "cons = get_hate_terms('./data/conservative.csv')\n",
    "ind = get_hate_terms('./data/independent.csv')\n",
    "labour =get_hate_terms('./data/labour.csv')\n",
    "lib_dem = get_hate_terms('./data/lib_dem.csv')\n",
    "\n",
    "terms = cons + ind + labour + lib_dem\n",
    "\n",
    "print(len(terms), terms)\n",
    "time_limit = 15 * 60\n",
    "jfile = open('./data/ua3.json', 'a')\n",
    "\n",
    "api = tweepy.API(authorization, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, retry_delay=5)\n",
    "for i in terms:\n",
    "    try:\n",
    "        c = tweepy.Cursor(api.search, q=i, count=100, lang=\"en\", \n",
    "                          since=\"2019-12-9\", \n",
    "                          until=\"2019-12-17\").items()\n",
    "        print(i)\n",
    "        start_time = time.time() \n",
    "        for tweet in c:\n",
    "            json.dump(tweet._json, jfile)\n",
    "            jfile.write(\"%s\"%'\\n')\n",
    "        if (time.time() - start_time) < time.time()+time_limit:\n",
    "            #print('------------sleeping for 10 seconds------------------')\n",
    "            print('reached limit')\n",
    "            time.sleep(3)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__doc__) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
